---
title: "Exercise & HW Assignment: Dealing with High Dimensional Data (ML_HW8)"
author: Judy Fordjuoh
date: March 20, 2022
output: word_document
---
# Exercise 1: Feature selection using regularization methods

This exercise is *loosely* based on the following paper: Integration of an interpretable machine learning algorithm to identify early life risk factors of childhood obesity among preterm infants: a prospective birth cohort https://doi.org/10.1186/s12916-020-01642-6. The data used in this exercise are an altered version of data available from the HHEAR Data Center, with dois https://doi.org/10.36043/2017-1740_EPI_58 and https://doi.org/10.36043/2017-1740_MAIN_84


In this exercise, you will utilize the caret package to optimize a regularization algorithm for feature selection. You will compare results when you include variables that could induce confounding as features entered into the algorithm.  You will also consider how study design and source of data can impact the conclusions drawn by a machine learning analysis. 

***

### Description of the Theoretical Study and Data

The goal of this study is to identify prenatal social and environmental risk factors for childhood overweight/obesity among preterm infants.This study is a prospective birth cohort involving mother-child pairs. Women were enrolled during the first or second trimester of pregnancy and were followed up via visiting clinics until the birth of their children. Women and children were then followed up periodically during infancy and childhood. A total of 1447 singleton children were born preterm, prior to 37 weeks gestation and had complete data on maternal demographics and pregnancy, birth characteristics, lifestyle factors, biospecimen analyzed for exposure to metals and to define childhood obesity at age 5. You have recently been hired as a research data analyst, and tasked with performing the analysis for this study.You are provided with a dataset containing a number of features, in addition to a binary outcome indicating childhood overweight or obesity vs normal weight. 

Features in the dataset have informative names. The following categorical features use codes to indicate the different labels:

Child.Human.Biological.Sex: sex assigned at birth of child; 
110:Female
111:Male

HHIncome: Household income during pregnancy; 
159: <$5,000
204:$5,000-$10,000, 
205: $10,000-$20,000, 
206: $20,000-$40,000, 
207: $40,000-$70,000, 
208: >$70,000

Race_ethnicity: Race or ethnicity of child, as reported by parent;
47:"Hispanic or Latino Ethnicity"
54:"Multiracial"
210:"Black Non-Hispanic"
212:"American Indian Non-Hispanic"
214:"White Non-Hispanic"
217:"Asian Non-Hispanic"
855824:"Other race/ethnicity than white/black/hispanic/asian/american indian/multiracial"

Mother_Education: Highest Educational Attainment at time of Pregnancy
4:"Advanced Graduate Degree"
12:"College Graduate"
32:"Graduated From High School"
203:"Some College or Technical School"
215:"Less than High School"

Smoking_Preg: maternal smoking during pregnancy
1: No smoking during pregnancy
2: Active smoker during pregnancy
81: Quit smoking before pregnancy

ow_obesity: overweight or obesity during childhood
1: Overweight or Obese >= 85th percentile
0: Typical weight <85th percentile (no underweight children in sample)

***

## BEFORE DATA ANALYSIS
### Question 1: What additional information, if any, would you want from the principal study investigators in regards to the above features? 
#### I would love to know why they chose the above features and how they decided to come up with the way they measured or the classifications they created for each feature. I would also like to know if theyh considered measuring other factors such as gestational weight gain, maternal overweight/obesity, access to healthy food during pregnancy, and access to prenatal care, as these are all characteristics that could be related to the outcome of childhood obesity.

### Question 2: Look at the features in the dataset before you start your analysis. Are there any you want to exclude from your analysis completely? Why or why not? Are there any you want to recode or transform? Why or why not?
#### I would recode the household income into quintiles (from lowest income to highest income). I would do this so that approximately 20% of the sample/population is in each group. This makes it easier to compare between groups in this study. Equally important, based on the amount of participants in each quintile, it will provide a better understanding of the socioeconomic status' represented in the study. Immediately exclude the features that measure specific RBC concentration of elements in the participants pregnancy sample. Some elements like lead may be interesting to analyze because if there are high levels, it can give us some insight on how the participant was living  (i.e. if they are living in a poorer neighborhood with buildings with old lead paint etc.). However, overall I do not think including the element features will provide much value in our study of the development of childhood obesity. 


### Question 3: Are any of the features not of interest as modifiable contributors to childhood overweight/obesity themselves, but in an explanatory model, you would typically include them? Will you include them in your analysis?
#### Features such as race/ethnicity of child is not an interest as a modifiable contributor to childhood overweight/obesity itself. Race is a social construct and based on my limited knowledge, certain races are not genetically more or less predisposed to childhood obesity in comparison to others. However, I would still include it in my initial model since we know that certain races and communities of color have been systematically oppressed and the effects of this can influence the rate of childhood obesity. 

## STEP ONE: Load Packages and Prepare Data
You will start by loading the needed packages. Some are already listed, but you can choose to use different ones. You will need to clean the data, check that values are plausible, ensure that all variables are the correct type for the algorithm and packages you want to use, etc.

```{r data_prep, results='hide'}
library(tidyverse)
library(caret)
library(glmnet)
library(Amelia)

library(dplyr)
library(readxl)
library(knitr)
library(gbm)
library(e1071)
library(rpart)
library(pROC)
```


```{r data prep, results='hide'}
set.seed(300)

bc = read.csv("/Users/judyfordjuoh/Desktop/Machine Learning/birthcohort_data.csv") %>%
  janitor::clean_names() %>% 
  select(-starts_with("pregnancy")) %>% 
  mutate(ow_obesity = as.factor(ow_obesity)) %>%
  mutate(ow_obesity = recode(ow_obesity,
                            "0" = "Typical Weight",
                            "1" = "Overweight/Obese")) 

#No ID variable to stripping off 

#Check distributions, missing data etc, omitting the NAs
summary(bc)
missmap(bc, main = "Missing values vs observed")
#Since there is no missing data we won't do na.omit(bc)

summary(bc$ow_obesity) #Notice that the data is unbalanced so we will have to upsample (increasing your minority sample) or downsample(decreasing your majority sample). Since my sample size is small, I will upsample.


```

## STEP TWO: Decide on a pipeline

### Question 4: In previous exercises, we often partition our sample into training and testing. We optimize hyperparameters using cross-validation. Is this pipeline still necessary if our goal is feature selection and not building a prediction model to apply to new data? What do you think? 
#### It is still necessary to perform cross validation even if our goal is feature selection and not building a prediction model. Cross-validation is a means of estimating the performance of a method for fitting a model, rather than of the model itself, so all steps in fitting the model (including feature selection and optimizing the hyper-parameters) need to be performed independently in each fold of the cross-validation procedure. If you don't do this, then you will end up with an optimistically biased performance estimate. 

Regardless of your answer above, partition the data into a 70/30 split just to get the practice with the programming code to partition.

```{r partition}
#tidyverse way to create data partition
train_indices <- createDataPartition(y = bc$ow_obesity,p = 0.7,list = FALSE)
train_data <- bc[train_indices, ]
test_data <- bc[-train_indices, ]
```

## STEP THREE: Construct a model using a regularization algorithm (lasso, ridge, elastic net) and the features of interest in the training data

### Question 5: Which regularization algorithm seems most appropriate for this research question? Justify your choice. 
#### Q5 Ans: I will use LASSO. If we use LASSO, which is L1 regularization, some of the features can shrink down to a zero. This basically works as feature selection. I would not recommend use a ridge regression because with ridge, the features can shrink down but itâ€™ll never be able to shrink it to zero, so I would retain all the features in the dataset. 

### Question 6: Which metrics will you use to evaluate your model? Consider your research question and the outcome of interest. 
#### Q6 Ans: Since our outcome is binary we will use the confusion matrix/accuracy to evaluate our model. If we were using a continuous outcome/linear regression model, we would use postResample.

Assess how the metric(s) change(s) based on values of the hyperparameters. Construct a grid to explore various values (do not just use the default parameters). Once you have a final model, determine the features that are considered "important" based on the model output.

 
```{r las}

#NTS: first create a grid to search lambda
lambda <- 10^seq(-5,5, length = 100)

set.seed(200)

#NTS: replace tuneLength with tuneGrid and alpha is 1 because we are doing lasso. If we were doing rigid it would be 0. 
las <- train(
  ow_obesity ~., data = train_data, method = "glmnet", trControl = trainControl("cv", number = 10, sampling = "up"), preProc = c("center", "scale"), tuneGrid = expand.grid(alpha = 1, lambda = lambda)
)

#Print the values of alpha and lambda that gave best prediction
las$bestTune %>% knitr::kable() # 1(alpha)|4.04 e-0.05(lambda)|0.9803 (Accuracy)
```

```{r LASSO, results='hide'}
#Print all of the options examined
las$results %>% knitr::kable()

# Model coefficients
coef(las$finalModel, las$bestTune$lambda)
```

```{r LASSO cont}
#Confusion Matrix
confusionMatrix(las) 
```
#### Based on the training data model, the two features that are important are birth weight and the infant growth Z-score with a beta of 31.61 and 30.72, respectively.The accuracy in this model is 0.9803. None of the features in this model went to 0. This could be because I removed the variables that were measures of elements in RBC.   

### STEP FOUR: Test your final model in the testing dataset
Use the implementation of your model in the testing set to obtain final performance metrics and perform the inference needed to address your research question. 

```{r}
#Using the test data to make predictions

las2 <- las %>% predict(test_data)
confusionMatrix(las2,test_data$ow_obesity, positive = "Overweight/Obese")

#Obtain predicted probabilities
test.outcome.probs <- predict(las, test_data, type = "prob")

testProbs.rmodel <- data.frame(obs = test_data$ow_obesity,
                        pred.las = test.outcome.probs[,2])

#Create calibration plot
obesity_PlotData.rmodel <- calibration(obs ~ pred.las, data = testProbs.rmodel, class = "Overweight/Obese", cuts = 5)

xyplot(obesity_PlotData.rmodel, auto.key = list(columns = 2))

plot(test.outcome.probs[,2])
```

###  Question 7: Summarize your final conclusion in 2-3 sentences
#### Q7 Ans:The accuracy of the model on the test data was 98.00 % which was similar to the accuracy of the model on the training data. Along with the 98% accuracy, there was a sensitivity of 0.9902 and a specificity of 0.9758.

### STEP FIVE: Construct another model, making a different choice of variable inclusion. 

Redo the above, but now make the opposite choice about variable inclusion. That is, if you did not include the features that themselves might not be modifiable contributors to childhood overweight/obesity, but you would typically include in an explanatory model, include them now. Conversely, if you include those variables previously, exclude them now

Question 7: Do the "important" features change when you make a differnt choice about the other features? Do the hyperparameters that optimize the model change when the additional variables are included? What about model performance?

```{r lasso with exclusion}
lambda <- 10^seq(-5,5, length = 100)

set.seed(200)

#NTS: replace tuneLength with tuneGrid and alpha is 1 because we are doing lasso. If we were doing rigid it would be 0. 
exclusion_lasso <- train(
  ow_obesity ~ birth_weight + infant_growth_zscore + gestational_length + birth_length + child_height + hh_income + mother_age + mother_bmi_pre_preg + mother_education + smoking_preg + child_human_biological_sex, data = train_data, method = "glmnet", trControl = trainControl("cv", number = 10, sampling = "up"), preProc = c("center", "scale"), tuneGrid = expand.grid(alpha = 1, lambda = lambda)
)

#Print the values of alpha and lambda that gave best prediction
exclusion_lasso$bestTune %>% knitr::kable() # 1(alpha)|4.04 e-0.05(lambda)|0.9803 (Accuracy)
```

```{r Lasso with exclusion, results='hide'}
#Print all of the options examined
exclusion_lasso$results %>% knitr::kable()

# Model coefficients
coef(exclusion_lasso$finalModel, exclusion_lasso$bestTune$lambda)
```

```{r exclusion_lasso cont}
#Confusion Matrix
confusionMatrix(exclusion_lasso) 
```
#### In this LASSO model, I included all the variables in my dataset except for race/ethnicity. Birth weight and the infant growth Z-score remained two most important variables and betas stayed the same roughly. The betas for birth weight and  infant growth Z-score increased slighty from 31.61 and 30.72 to 31.68 and 30.78, respectively. The accuracy of this LASSO model was 98%, which shows that there was not a major change in the accuracy of the model even if race was not included in the model. 

# Exercise 2: Creating more refined phenotypes for an explanatory analysis.

This exercise is *loosely* based on the following paper: Deploying unsupervised clustering analysis to derive clinical phenotypes and risk factors associated with mortality risk in 2022 critically ill patients with COVID-19 in Spain doi:10.1186/s13054-021-03487-8. Data were simulated and are not true COVID data.

Researchers are interested in understanding the factors associated with ICU mortality among COVID-19 patients. They hypothesize there are different clinical phenotypes that could be at different risks for mortality and require different medical interventions. The goal of this research is to determine if patient features including demographics and clinical data at ICU admission could be used to separate COVID-19 patients into distinct phenotypic clusters. The secondary aim was to determine if identified phenotypic clusters had different risk of mortality. 

You are provided with the following dataset for a subset of 178 COVID-19 patients, with the instructions to conduct an unsupervised analysis to identify phenotypic clusters within the patient population, describe the clusters in terms of the input features and then determine if there are differences in mortality rate across the clusters. All feature data in the dataset have been centered and scaled. The outcome, mortality, is a binary indicator.

Variables in the dataset (covid.csv) are:

Ageyr: the age of the patient at admission to the ICU
APACHE:APACHE II Score, Acute Physiology and Chronic Health Evaluation II Score
SOFA: SOFA Score Sequential Organ Failure Assessment
DDimer: D dimer, a fibrin degradation product, can indicate blood clot formation and breakdown
SerumLactate: measures level of lactic acid in the blood, can be indicator of hypoxia
Ferritin: measure of iron sufficiency in blood
CRP: C-reactive protein, measure of inflammation
Creatinine: product of protein metabolism, high levels can indicate impaired kidney function
WBC: concentration of white blood cells, marker of infection
DBP: diastolic blood pressure
Procalcitonin: marker of bacterial infection
IGA: immunoglobulin A, measure of antibodies found in mucous membranes
Oxmetric.1: measure of oxygen saturation
mortality: 1=Died in ICU, 0=Survived

***

### STEP 1: Load needed libraries and implement the unsupervised analysis

```{r}
library(stats)
library(cluster)
library(factoextra)

covid = read.csv("/Users/judyfordjuoh/Desktop/Machine Learning/covid.csv") %>%
  janitor::clean_names() 
```
### Question 1: Name one potential risk/concern of obtaining data that has already been centered and scaled.
#### Q1 Answer: The issue is what represents a good measure of distance between cases.

Centering and scaling the data is a process by which you transform each feature such that its mean becomes 0, and variance becomes 1. If you center and scale your data before splitting it into training and test sets, then you have used information from your training set to make calculations on your test set. The issue with having data that is already centered and scaled is the measure of distance between cases. If each feature is different/has different units (ex. ft and lb), these features are not comparable. So if we clustered individuals their height in feet and clustered individuals by their weight in lbs, is a 1ft difference as significant as a 1lb difference in weight?


### Question 2: What unsupervised analysis do you think is appropriate for this research question? Justify your answer.
#### Q2 Answer: Hieracrhical clustering is an appropriate unsupervised analysis approach because we are trying to identify clusters based on similarity across features. We would not do a principal components analysis because it is a form of dimensionality reduction, which is based on reducing the number of variables or features under consideration. Dimensionality reduction can either be feature selection or feature extraction, not for identifiying clusters. K-means is an inappropriate approach because our outcome is binary. Since K-means needs to compute means, if we have binary values, discrete attributes or categorial attributes, we tend to stay away from k-means since the mean value is not meaningful on this kind of data. 

Implement the unsupervised analysis you chose in Question 2, implementing appropriate analyses to determine the number of phenotypic clusters to keep within the analysis.

```{r}
#Determining if scaling is necessary. Since murder is really low, I'll scale the data
colMeans(covid, na.rm = TRUE) #the other variables are e-16 while mortality is e-01 so I think I don't have to scale #apply(covid, 2, sd, na.rm=TRUE)

clusters_hcut <- hcut(covid, k = 2, hc_func = "hclust", hc_method = "complete", hc_metric = "euclidian")

clusters_hcut$size
fviz_dend(clusters_hcut, rect = TRUE)
fviz_cluster(clusters_hcut)

gap_stat <- clusGap(covid, FUN = hcut, hc_method = "complete", K.max = 10, B = 5)
fviz_gap_stat(gap_stat)

input.feature.vals <- cbind(covid,cluster = clusters_hcut$cluster)

input.feature.vals %>%
  group_by(cluster) %>%
  summarise_all(mean) %>% knitr::kable()

```

### STEP 2: Interpret the clusters

Describe the clusters in terms of both their input features and the incidence of mortality within the cluster.

#### I used Euclidean as my distance metric and complete linkage so that I can use the distance between all of the different data points. After visualizing the gapstat, the optimal number of clusters was 2. 

#### In the first cluster there were 94 participants and the second cluster had 84 participants.  In cluster one, the average mortality was 0.2127660 while in cluster two it was 0.130954. Overall in cluster two, the average of all the features were all larger than the averages of the same features in cluster one. For instance, the measure of oxygen saturation was 0.70 in cluster two but in cluster one it was 0.6. We see that all the features averages are larger in cluster 2 except for mortality, which was larger in cluster one. 



### Question 3: A researcher at a different medical institution has heard about your analysis and is interested in using your results to determine risk of mortality within their ICU. What are some limitations or concerns of using the results from your unsupervised analysis in a different setting?

#### The clusters formed in the data in my analysis may not align with the clusters that would form in a different setting. Therefore, the assumptions or inferences the researcher would make about risk of mortality within their ICU based on my results, may potentially be biased because the individuals in their ICU may have different, since their feature data may differ.
